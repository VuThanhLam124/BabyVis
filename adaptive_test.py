#!/usr/bin/env python3
"""
Test adaptive VRAM configuration for different environments
"""
import os
import sys
import time

sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_adaptive_configuration():
    """Test adaptive configuration detection"""
    try:
        from babyvis.model_utils import _detect_vram_gb, _get_adaptive_config
        
        vram_gb = _detect_vram_gb()
        config = _get_adaptive_config()
        
        print(f"üéØ Adaptive Configuration Test")
        print(f"=" * 50)
        print(f"üìä Detected VRAM: {vram_gb:.1f}GB")
        print(f"üé™ Profile: {config['profile'].upper()}")
        print(f"‚öôÔ∏è Settings:")
        print(f"   - 4-bit quantization: {config['use_4bit']}")
        print(f"   - 8-bit quantization: {config['use_8bit']}")
        print(f"   - Flash Attention: {config['flash_attention']}")
        print(f"   - GPU Layers (GGUF): {config['gpu_layers']}")
        print(f"   - Batch Size: {config['batch_size']}")
        print(f"   - Max Length: {config['max_length']}")
        
        # Provide recommendations
        print(f"\nüí° Recommendations for your {config['profile']} setup:")
        
        if config['profile'] == 'server':
            print("   üöÄ Maximum performance mode!")
            print("   ‚úÖ Use direct Qwen Image Edit with full precision")
            print("   ‚úÖ Flash Attention enabled for speed")
            print("   ‚úÖ Can process multiple images in parallel")
            print("   üìã Command: export USE_QWEN_IMAGE_EDIT=1")
            
        elif config['profile'] == 'personal':
            print("   ‚öñÔ∏è Balanced performance for 4GB VRAM")
            print("   ‚úÖ Use 4-bit quantized Qwen Image Edit")
            print("   ‚úÖ Optimized memory management")
            print("   ‚úÖ Single image processing recommended")
            print("   üìã Commands:")
            print("      export USE_QWEN_IMAGE_EDIT=1")
            print("      export QWEN_4BIT=true")
            
        elif config['profile'] == 'minimal':
            print("   üíæ Memory-efficient mode for limited VRAM")
            print("   ‚úÖ Use GGUF quantized models")
            print("   ‚úÖ Minimal GPU layers, CPU fallback")
            print("   ‚úÖ Most memory efficient")
            print("   üìã Commands:")
            print("      export USE_QWEN_GGUF=1")
            print("      export QWEN_N_GPU_LAYERS=8")
            
        else:  # cpu
            print("   üñ•Ô∏è CPU-only processing")
            print("   ‚úÖ Use ControlNet stable diffusion")
            print("   ‚úÖ Reliable but slower processing")
            print("   üìã Commands:")
            print("      export FORCE_CPU=1")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Configuration test failed: {e}")
        return False

def test_environment_profiles():
    """Test different environment profile simulations"""
    profiles = [
        ('Server (10GB VRAM)', 10.0),
        ('Personal (4GB VRAM)', 4.0), 
        ('Minimal (2GB VRAM)', 2.0),
        ('CPU Only (0GB VRAM)', 0.0)
    ]
    
    print(f"\nüß™ Testing Different Environment Profiles")
    print(f"=" * 50)
    
    # Mock different VRAM scenarios
    original_cuda_available = None
    try:
        import torch
        original_cuda_available = torch.cuda.is_available
        
        for name, vram_gb in profiles:
            print(f"\nüéØ Simulating: {name}")
            print(f"   VRAM: {vram_gb}GB")
            
            # Mock VRAM detection
            def mock_cuda_available():
                return vram_gb > 0
            
            def mock_get_device_properties(device_id):
                class MockProps:
                    def __init__(self, memory_gb):
                        self.total_memory = int(memory_gb * 1024**3)
                        self.name = f"Mock GPU ({memory_gb}GB)"
                return MockProps(vram_gb)
            
            # Temporarily patch torch functions
            torch.cuda.is_available = mock_cuda_available
            if vram_gb > 0:
                torch.cuda.get_device_properties = mock_get_device_properties
            
            from babyvis.model_utils import _detect_vram_gb, _get_adaptive_config
            
            # Force reload of config
            detected_vram = _detect_vram_gb()
            config = _get_adaptive_config()
            
            print(f"   Profile: {config['profile']}")
            print(f"   4-bit: {config['use_4bit']}, 8-bit: {config['use_8bit']}")
            print(f"   GPU Layers: {config['gpu_layers']}")
            
    except Exception as e:
        print(f"‚ö†Ô∏è Profile simulation failed: {e}")
    
    finally:
        # Restore original functions
        if original_cuda_available:
            try:
                import torch
                torch.cuda.is_available = original_cuda_available
            except:
                pass

def main():
    print("üîß BabyVis Adaptive VRAM Configuration Test")
    print("=" * 60)
    
    # Test current configuration
    if test_adaptive_configuration():
        print("\n‚úÖ Adaptive configuration working!")
    else:
        print("\n‚ùå Configuration test failed")
        return
    
    # Test different profile simulations
    test_environment_profiles()
    
    print(f"\nüöÄ Ready to run BabyVis with adaptive settings!")
    print(f"   python3 apps/batch_processor.py")
    
    print(f"\nüìñ Manual overrides available:")
    print(f"   Server mode:   export USE_QWEN_IMAGE_EDIT=1 && export QWEN_4BIT=false")
    print(f"   Personal mode: export USE_QWEN_IMAGE_EDIT=1 && export QWEN_4BIT=true") 
    print(f"   GGUF mode:     export USE_QWEN_GGUF=1")
    print(f"   CPU mode:      export FORCE_CPU=1")

if __name__ == "__main__":
    main()