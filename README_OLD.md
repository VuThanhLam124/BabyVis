# ğŸ¼ BabyVis - Professional Ultrasound to Baby Face Generation# BabyVis - GGUF Edition# BabyVis - GGUF Edition# ğŸ¤– BabyVis - Qwen Edition# BabyVis â€” Táº¡o áº£nh em bÃ© tá»« áº£nh siÃªu Ã¢m



**AI-powered ultrasound analysis and baby face prediction using Qwen-Image-Edit with Diffusers**



[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)ğŸ¤– **AI-powered ultrasound to baby face visualization using Qwen Image Edit GGUF model**

[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

[![GPU](https://img.shields.io/badge/GPU-4GB%20VRAM%20Optimized-green.svg)]()



## ğŸš€ Quick Start## ğŸš€ Quick StartğŸ¤– **AI-powered ultrasound to baby face visualization using Qwen Image Edit GGUF model**



### Option 1: Automatic Installation (Recommended)

```bash

# Clone repository### Option 1: App Interface (Recommended)

git clone <your-repo>

cd BabyVis```bash



# Install optimized for your hardware./run_app.sh## ğŸš€ Quick Start**Dá»± Ä‘oÃ¡n hÃ¬nh áº£nh em bÃ© tá»« siÃªu Ã¢m sá»­ dá»¥ng Qwen Image Edit AI**BabyVis lÃ  má»™t cÃ´ng cá»¥ táº¡o áº£nh em bÃ© tá»« áº£nh siÃªu Ã¢m dá»±a trÃªn Stable Diffusion + ControlNet (Canny), kÃ¨m giao diá»‡n Gradio thÃ¢n thiá»‡n vÃ  cháº¿ Ä‘á»™ xá»­ lÃ½ theo lÃ´.

./install_optimized.sh

```

# Run with sample images

python main.py --input samples/ --output outputs/baby_faces/Choose from:

```

- ğŸ–¥ï¸ **Desktop App** - Native GUI with drag & drop

### Option 2: Manual Installation

```bash- ğŸŒ **Web Interface** - Browser-based interface ### Option 1: GPU 4GB VRAM

# Install dependencies

pip install -r requirements.txt- ğŸ“¦ **Batch Processing** - Process multiple images



# Single image processing- ğŸš€ **App Launcher** - GUI selector```bash

python main.py --input samples/1.jpeg --output outputs/ --prompt "realistic baby face"



# Batch processing

python main.py --input samples/ --output outputs/baby_faces/### Option 2: Hardware-Specific Scripts./run_4gb.sh[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)## CÃ i Ä‘áº·t

```



## ğŸ¯ Features

**GPU 4GB VRAM:**```

- âœ… **Professional Diffusers Pipeline**: Uses standard `DiffusionPipeline.from_pretrained("Qwen/Qwen-Image-Edit")`

- âœ… **4GB VRAM Optimized**: Quantized models, CPU offloading, memory-efficient attention```bash

- âœ… **Auto Hardware Detection**: Automatically chooses optimal configuration for your hardware

- âœ… **Batch Processing**: Process multiple ultrasound images at once./run_4gb.sh[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)- YÃªu cáº§u Python 3.9+

- âœ… **Memory Management**: Advanced VRAM optimization with cleanup and monitoring

- âœ… **Production Ready**: Clean, modular codebase with proper error handling```



## ğŸ“‹ Requirements### Option 2: GPU 12GB VRAM  



### Minimum Hardware**GPU 12GB VRAM:**

- **GPU**: NVIDIA RTX 3050 Ti (4GB VRAM) or better

- **CPU**: Intel i5/AMD Ryzen 5 (for CPU-only mode)```bash```bash- CÃ i thÆ° viá»‡n:

- **RAM**: 8GB system memory

- **Storage**: 10GB free space (for model cache)./run_12gb.sh



### Software```./run_12gb.sh

- Python 3.8+

- CUDA 11.8+ (for GPU acceleration)

- PyTorch 2.0+

- Diffusers 0.21+**CPU Only:**```## ğŸš€ TÃ­nh nÄƒng  - `pip install -r requirements.txt`



## âš™ï¸ Hardware Configurations```bash



| Configuration | VRAM Usage | Speed | Quality | Recommended For |./run_cpu.sh

|---------------|------------|-------|---------|-----------------|

| **Quantized GPU** | ~3.5GB | Fast | High | RTX 3050 Ti, GTX 1660 |```

| **Full GPU** | ~6-8GB | Fastest | Highest | RTX 3060+, RTX 4060+ |

| **CPU Only** | 0GB | Slower | Good | No GPU available |### Option 3: CPU Only



## ğŸ“ Project Structure## ğŸ“‹ Requirements



``````bash

BabyVis/

â”œâ”€â”€ main.py                     # Main application entry point- Python 3.8+

â”œâ”€â”€ install_optimized.sh        # Optimized installation script

â”œâ”€â”€ requirements.txt            # Dependencies- For GPU: NVIDIA GPU with 4GB+ VRAM./run_cpu.sh- **ğŸ§  Pure Qwen AI**: Sá»­ dá»¥ng hoÃ n toÃ n Qwen Image Edit - loáº¡i bá» ControlNet Ä‘á»ƒ hiá»‡u suáº¥t tá»‘i Æ°u## Cháº¡y nhanh (Gradio UI)

â”œâ”€â”€ src/babyvis/               # Core processing modules

â”‚   â”œâ”€â”€ __init__.py- For CPU: 8GB+ RAM

â”‚   â”œâ”€â”€ processor.py           # Main processor class

â”‚   â”œâ”€â”€ inference.py           # Inference pipeline```

â”‚   â”œâ”€â”€ batch_processor.py     # Batch processing utilities

â”‚   â”œâ”€â”€ model_downloader.py    # Model management## ğŸ”§ Setup

â”‚   â””â”€â”€ model_utils.py         # Model utilities

â”œâ”€â”€ samples/                   # Sample ultrasound images- **âš¡ Auto-Detection**: Tá»± Ä‘á»™ng chá»n backend tá»‘t nháº¥t (GGUF hoáº·c Transformers)- Cháº¡y giao diá»‡n: `python apps/gradio_app.py`

â”œâ”€â”€ outputs/                   # Generated baby faces

â”‚   â”œâ”€â”€ diffusers_baby_faces/  # Main output directory1. **Clone the repository:**

â”‚   â””â”€â”€ final_intelligent/     # Previous analysis results

â””â”€â”€ models/                    # Model cache (auto-created)```bash## ğŸ“‹ Requirements

```

git clone <your-repo>

## ğŸ› ï¸ Usage Examples

cd BabyVis- **ğŸ”§ Adaptive VRAM**: Tá»± Ä‘á»™ng tá»‘i Æ°u theo VRAM cÃ³ sáºµn (4GB personal â†’ 10GB+ server)- Quy trÃ¬nh: Táº£i áº£nh siÃªu Ã¢m â†’ Nháº¥n â€œBáº¯t Ä‘áº§u xá»­ lÃ½â€ â†’ Táº£i áº£nh káº¿t quáº£.

### Command Line Interface

```

**Single Image:**

```bash- Python 3.8+

python main.py \

  --input samples/ultrasound.jpg \2. **Add ultrasound images:**

  --output outputs/ \

  --prompt "Transform this ultrasound into a beautiful realistic baby face" \```bash- For GPU: NVIDIA GPU with 4GB+ VRAM- **ğŸ¯ High Performance**: Tá»‘i Æ°u hÃ³a cho tá»‘c Ä‘á»™ vÃ  cháº¥t lÆ°á»£ng- áº¢nh káº¿t quáº£ máº·c Ä‘á»‹nh lÆ°u táº¡i: `outputs/gradio/`

  --steps 25 \

  --guidance 7.5mkdir -p samples

```

# Copy your ultrasound images to samples/ folder- For CPU: 8GB+ RAM

**Batch Processing:**

```bash```

python main.py \

  --input samples/ \- **ğŸ’» Cross-Platform**: Há»— trá»£ GPU CUDA, CPU, vÃ  cÃ¡c mÃ´i trÆ°á»ng hybrid

  --output outputs/baby_faces/ \

  --model-variant quantized \3. **Run with your preferred interface:**

  --cpu-offload

``````bash## ğŸ”§ Setup



**Advanced Options:**# App interface (recommended)

```bash

python main.py \./run_app.sh## Xá»­ lÃ½ theo lÃ´ (Batch)

  --input samples/ \

  --output outputs/ \

  --device cuda \

  --model-variant quantized \# Or specific hardware config1. **Clone the repository:**

  --steps 30 \

  --guidance 8.0 \./run_4gb.sh    # For 4GB GPU

  --cpu-offload

```./run_12gb.sh   # For 12GB GPU  ```bash## ğŸ“¦ CÃ i Ä‘áº·t nhanh- áº¢nh máº«u náº±m trong `samples/`



### Python API./run_cpu.sh    # For CPU only



```python```git clone <your-repo>

from src.babyvis.processor import OptimizedBabyVisProcessor



# Initialize processor

processor = OptimizedBabyVisProcessor(enable_cpu_offload=True)## ğŸ“ Project Structurecd BabyVis- CÃ³ thá»ƒ chuáº©n bá»‹ danh sÃ¡ch áº£nh táº¡i `data/image_list.txt` (má»—i dÃ²ng 1 Ä‘Æ°á»ng dáº«n)



# Load quantized model for 4GB VRAM

processor.load_model("quantized")

``````

# Process single image

result = processor.process_ultrasound_to_baby(BabyVis/

    "samples/ultrasound.jpg",

    prompt="realistic baby face from ultrasound",â”œâ”€â”€ run_app.sh         # App interface launcher```bash- Cháº¡y: `python apps/batch_processor.py`

    num_inference_steps=25

)â”œâ”€â”€ run_4gb.sh         # GPU 4GB configuration



# Save resultâ”œâ”€â”€ run_12gb.sh        # GPU 12GB configuration  2. **Add ultrasound images:**

if result:

    result.save("output/baby.png")â”œâ”€â”€ run_cpu.sh         # CPU configuration



# Batch processâ”œâ”€â”€ app_desktop.py     # Desktop GUI app```bash# Clone repo- Káº¿t quáº£: `outputs/batch/`; Log: `logs/batch.log`

outputs = processor.batch_process(

    "samples/", â”œâ”€â”€ app_web.py         # Web interface app

    "outputs/baby_faces/",

    prompt="beautiful baby face prediction"â”œâ”€â”€ app_launcher.py    # GUI app selectormkdir -p samples

)

â”œâ”€â”€ batch_processor.py # Batch processing

print(f"Generated {len(outputs)} baby faces")

â”œâ”€â”€ model_downloader.py # Auto-download GGUF model# Copy your ultrasound images to samples/ foldergit clone https://github.com/VuThanhLam124/BabyVis.git

# Check memory usage

memory_info = processor.get_memory_usage()â”œâ”€â”€ samples/           # Input ultrasound images

print(f"VRAM usage: {memory_info}")

â”œâ”€â”€ outputs/           # Generated baby faces```

# Cleanup

processor.cleanup()â”œâ”€â”€ models/           # GGUF model storage (auto-created)

```

â””â”€â”€ src/babyvis/      # Core inference codecd BabyVis## Cháº¡y trÃªn CPU (kháº¯c phá»¥c lá»—i CUDA/cuBLAS)

## ğŸ”§ Configuration Options

```

### Model Variants

- `default`: Full precision model (6-8GB VRAM)3. **Run with your preferred configuration:**

- `quantized`: 4-bit quantized model (3-4GB VRAM) - **Recommended for RTX 3050 Ti**

- `cpu`: CPU-optimized model## ğŸ–¥ï¸ App Interfaces



### Performance Tuning```bash- Náº¿u gáº·p lá»—i kiá»ƒu `libcublasLt.so.11`/`CUDA` hoáº·c chÆ°a cÃ i driver/phá»¥ thuá»™c CUDA Ä‘Ãºng báº£n:

```bash

# For 4GB VRAM (RTX 3050 Ti)### Desktop App (app_desktop.py)

python main.py --model-variant quantized --cpu-offload --steps 20

- **Native GUI** with Tkinter# For 4GB GPU

# For 8GB+ VRAM  

python main.py --model-variant default --steps 30- **Drag & drop** image selection



# CPU-only mode- **Real-time preview** of input/output./run_4gb.sh# Táº¡o conda environment  - Táº¡m thá»i Ã©p cháº¡y CPU: `CUDA_VISIBLE_DEVICES="" python apps/batch_processor.py` hoáº·c `FORCE_CPU=1 python apps/batch_processor.py`

python main.py --device cpu --model-variant cpu --steps 15

```- **Progress bar** and status updates



## ğŸ“Š Performance Benchmarks- **Save functionality** with file dialog



### RTX 3050 Ti (4GB VRAM)

- **Quantized model**: ~25-35 seconds per image

- **VRAM usage**: ~3.2GB peak### Web Interface (app_web.py)  # For 12GB GPU  conda create -n babyvis python=3.9 -y  - Code cÃ³ cÆ¡ cháº¿ tá»± fallback GPUâ†’CPU náº¿u lá»—i CUDA xáº£y ra trong lÃºc suy luáº­n.

- **Quality**: High (comparable to full model)

- **Browser-based** interface using Gradio

### RTX 3060 (12GB VRAM)

- **Full model**: ~15-20 seconds per image  - **Upload images** via web form./run_12gb.sh

- **VRAM usage**: ~6.5GB peak

- **Quality**: Highest- **Live processing** with progress



### CPU Only (Intel i7)- **Download results** directlyconda activate babyvis

- **Processing time**: ~2-4 minutes per image

- **RAM usage**: ~8GB peak- **Responsive design** for mobile/desktop

- **Quality**: Good

# For CPU only

## ğŸ› ï¸ Troubleshooting

### Batch Processing (batch_processor.py)

### Memory Issues

```bash- **Command-line** interface./run_cpu.sh## Tuá»³ chá»n mÃ´ hÃ¬nh

# Out of VRAM error

python main.py --model-variant quantized --cpu-offload- **Process entire** samples/ folder



# Still running out of memory- **Automatic output** naming```

python main.py --device cpu --model-variant cpu

```- **Progress logging**



### Model Loading Issues# CÃ i Ä‘áº·t dependencies- Máº·c Ä‘á»‹nh SD v1.5 + ControlNet Canny (CPU náº¿u khÃ´ng cÃ³ GPU).

```bash

# Clear cache and retry## ğŸ¤– Model Information

rm -rf ~/.cache/huggingface/

python main.py --input samples/1.jpeg --output outputs/## ğŸ“ Project Structure

```

- **Model**: Qwen_Image_Edit-Q4_K_M.gguf (7.5GB)

### Performance Issues

```bash- **Source**: QuantStack/Qwen-Image-Edit-GGUFpip install -r requirements.txt- CÃ³ thá»ƒ Ä‘á»•i sang SDXL qua biáº¿n mÃ´i trÆ°á»ng hoáº·c override ID:

# Install XFormers for faster inference

pip install xformers- **Auto-download**: Model downloads automatically on first run



# Enable compilation (PyTorch 2.0+)```

export TORCH_COMPILE=1

python main.py --input samples/ --output outputs/## âš™ï¸ Hardware Configurations

```

BabyVis/  - VÃ­ dá»¥ SDXL: 

### CUDA Issues

```bash| Configuration | GPU Layers | VRAM Usage | RAM Usage | Speed |

# Check CUDA installation

python -c "import torch; print(torch.cuda.is_available())"|---------------|------------|------------|-----------|-------|â”œâ”€â”€ run_4gb.sh         # GPU 4GB configuration



# Force CPU mode if CUDA problematic| 4GB GPU       | 20         | ~3.5GB     | ~4GB      | Fast  |

python main.py --device cpu --input samples/ --output outputs/

```| 12GB GPU      | 35         | ~11GB      | ~2GB      | Fastest |â”œâ”€â”€ run_12gb.sh        # GPU 12GB configuration  # Cháº¡y ngay láº­p tá»©c    - `PREFER_SDXL=1 python apps/gradio_app.py`



## ğŸ” Memory Optimization Features| CPU Only      | 0          | 0          | ~8GB      | Slower |



- **CPU Offloading**: Automatically moves models between GPU/CPUâ”œâ”€â”€ run_cpu.sh         # CPU configuration

- **Attention Slicing**: Reduces peak memory usage

- **VAE Tiling**: Handles large images efficiently## ğŸ¯ Features

- **Automatic Cleanup**: Frees memory after each inference

- **Memory Monitoring**: Real-time VRAM usage trackingâ”œâ”€â”€ batch_processor.py # Main processing script./run_qwen_auto.sh    - Hoáº·c:

- **Gradient Checkpointing**: Trades compute for memory

- âœ… **Multiple interfaces** (Desktop, Web, Batch)

## ğŸ“ˆ Quality Enhancement Tips

- âœ… **Single GGUF model** (simplified)â”œâ”€â”€ model_downloader.py # Auto-download GGUF model

### Optimal Prompts

```python- âœ… **Auto-download** from HuggingFace

# Generic prompt

"Transform this ultrasound into a realistic baby face"- âœ… **Hardware auto-detection**â”œâ”€â”€ samples/           # Input ultrasound images```      - `BASE_MODEL_ID="stabilityai/stable-diffusion-xl-base-1.0" \\



# Detailed prompt for better results- âœ… **Real-time processing** with progress

"Transform this ultrasound image into a beautiful, realistic newborn baby face with soft features, clear skin, and natural proportions"

- âœ… **3 optimization profiles**â”œâ”€â”€ outputs/           # Generated baby faces

# Ethnicity-specific

"Transform this ultrasound into a realistic Asian baby face with delicate features"- âœ… **Simple configuration**

```

â”œâ”€â”€ models/           # GGUF model storage (auto-created)         CONTROLNET_ID="diffusers/controlnet-canny-sdxl-1.0" \\

### Parameter Tuning

- **Steps**: 20-30 (balance speed/quality)## ğŸ“Š Output

- **Guidance Scale**: 7.0-8.5 (higher = more prompt adherence)

- **Image Size**: 512x512 (optimal for memory)â””â”€â”€ src/babyvis/      # Core inference code



## ğŸ¯ RoadmapGenerated baby faces will be saved in the `outputs/` directory with filenames like:



- [ ] **Web Interface**: Gradio/Streamlit web interface- `predicted_1.png````## ğŸ® CÃ¡ch sá»­ dá»¥ng         python apps/gradio_app.py`

- [ ] **Mobile Support**: Optimized mobile processing

- [ ] **Advanced Features**: Gender prediction, trait analysis  - `predicted_2.png` 

- [ ] **Cloud Integration**: AWS/GCP deployment options

- [ ] **API Server**: REST API for integration- etc.

- [ ] **Real-time Processing**: Live camera feed support



## ğŸ“ License

## ğŸ› ï¸ Usage Examples## ğŸ¤– Model Information- CÃ¡c tham sá»‘ gá»£i Ã½:

MIT License - see [LICENSE](LICENSE) file for details.



## ğŸ¤ Contributing

### Desktop App

1. Fork the repository

2. Create feature branch: `git checkout -b feature/amazing-feature`1. Run `./run_app.sh` and choose "Desktop App"

3. Commit changes: `git commit -m 'Add amazing feature'`

4. Push to branch: `git push origin feature/amazing-feature`2. Click "Browse Image" to select ultrasound- **Model**: Qwen_Image_Edit-Q4_K_M.gguf (7.5GB)### Cháº¿ Ä‘á»™ tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)  - SD v1.5: steps 40â€“60, CFG 7â€“9, control 0.7â€“1.0

5. Open Pull Request

3. Choose hardware configuration

## ğŸ™ Acknowledgments

4. Click "Generate Baby Face"- **Source**: QuantStack/Qwen-Image-Edit-GGUF

- **Qwen Team**: For the amazing Qwen-Image-Edit model

- **Hugging Face**: For Diffusers library and model hosting5. Save result when processing complete

- **PyTorch Team**: For the deep learning framework

- **NVIDIA**: For CUDA acceleration support- **Auto-download**: Model downloads automatically on first run```bash  - SDXL: steps 30â€“40, CFG 5â€“7, control 0.6â€“0.9



## ğŸ“ Support### Web Interface



- **Issues**: [GitHub Issues](https://github.com/yourusername/BabyVis/issues)1. Run `./run_app.sh` and choose "Web Interface"

- **Discussions**: [GitHub Discussions](https://github.com/yourusername/BabyVis/discussions)

- **Email**: support@babyvis.ai2. Open browser to http://localhost:7860



---3. Upload ultrasound image## âš™ï¸ Hardware Configurations./run_qwen_auto.sh    # Tá»± Ä‘á»™ng chá»n backend tá»‘t nháº¥t



**ğŸš€ Transform ultrasounds into beautiful baby faces with professional AI technology!**4. Configure settings



**ğŸ’¡ Optimized for RTX 3050 Ti and 4GB VRAM setups - no compromises on quality!**5. Click "Generate" and download result



### Batch Processing| Configuration | GPU Layers | VRAM Usage | RAM Usage | Speed |```### DÃ¹ng Qwen Image Edit (Qwen/Qwen-Image-Edit)

1. Put ultrasound images in `samples/` folder

2. Run `./run_app.sh` and choose "Batch Processing"|---------------|------------|------------|-----------|-------|

3. All images processed automatically

4. Check `outputs/` folder for results| 4GB GPU       | 20         | ~3.5GB     | ~4GB      | Fast  |- Báº­t dÃ¹ng Qwen Image Edit thay cho Stable Diffusion + ControlNet:



## ğŸ› ï¸ Troubleshooting| 12GB GPU      | 35         | ~11GB      | ~2GB      | Fastest |



### Model Download Issues| CPU Only      | 0          | 0          | ~8GB      | Slower |### Cháº¿ Ä‘á»™ cá»¥ thá»ƒ  - `USE_QWEN_IMAGE_EDIT=1 python apps/gradio_app.py`

```bash

python3 -c "

from model_downloader import ensure_model

ensure_model()## ğŸ¯ Features```bash  - hoáº·c trong batch: `USE_QWEN_IMAGE_EDIT=1 python apps/batch_processor.py`

"

```



### Memory Issues- âœ… Single GGUF model (simplified)./run_qwen_gguf.sh         # Force GGUF (tÆ°Æ¡ng thÃ­ch cao)- YÃªu cáº§u cÃ i thÃªm thÆ° viá»‡n: `transformers`, `accelerate`, `sentencepiece`, `safetensors` (Ä‘Ã£ thÃªm sáºµn trong `requirements.txt`).

- Use CPU mode if GPU memory is insufficient

- Close other applications to free RAM- âœ… Auto-download from HuggingFace

- Try 4GB mode instead of 12GB mode

- âœ… Hardware auto-detection./run_qwen_transformers.sh # Force Transformers (cháº¥t lÆ°á»£ng cao)- Ghi chÃº: MÃ£ táº£i Qwen dÃ¹ng `trust_remote_code=True` theo hÆ°á»›ng dáº«n tá»« Hugging Face. Äáº§u ra áº£nh Ä‘Æ°á»£c decode tá»± Ä‘á»™ng tá»« response cá»§a model.

### GUI Issues

```bash- âœ… Batch processing

# For desktop app, ensure tkinter is installed

sudo apt-get install python3-tk  # Ubuntu/Debian- âœ… 3 optimization profiles./run_qwen_cpu.sh          # CPU only (mÃ¡y yáº¿u)



# For web interface, check gradio installation- âœ… Simple configuration

pip install gradio

``````### DÃ¹ng báº£n nháº¹ GGUF (QuantStack/Qwen-Image-Edit-GGUF)



### CUDA Issues## ğŸ“Š Output

```bash

# Check CUDA availability- Báº­t backend GGUF (cháº¡y báº±ng llama.cpp, phÃ¹ há»£p CPU/GPU VRAM ~4GB):

python3 -c "

import subprocessGenerated baby faces will be saved in the `outputs/` directory with filenames like:

try:

    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)- `predicted_1.png`### Python API  - `USE_QWEN_GGUF=1 python apps/gradio_app.py`

    print(result.stdout)

except:- `predicted_2.png` 

    print('CUDA not available, will use CPU')

"- etc.```python  - hoáº·c `USE_QWEN_GGUF=1 python apps/batch_processor.py`

```



## ğŸ“ License

## ğŸ› ï¸ Troubleshootingfrom babyvis.inference import generate_predict_auto- Tuá»³ chá»n táº£i model:

See LICENSE file for details.



## ğŸ¤ Contributing

### Model Download Issues  - DÃ¹ng file cá»¥c bá»™: Ä‘áº·t `QWEN_GGUF_PATH=/path/to/model.gguf`

This is a simplified GGUF-only version focused on ease of use and performance with multiple interface options.
```bash

python3 -c "# Táº¡o áº£nh dá»± Ä‘oÃ¡n  - Hoáº·c Ä‘á»ƒ tá»± táº£i tá»« HF: `QWEN_GGUF_REPO=QuantStack/Qwen-Image-Edit-GGUF` vÃ  (tuá»³ chá»n) `QWEN_GGUF_FILENAME=qwen-image-edit-q4_k_m.gguf`

from model_downloader import ModelDownloader

downloader = ModelDownloader()generate_predict_auto(- Tinh chá»‰nh hiá»‡u nÄƒng/thÃ´ng sá»‘ tháº¥p VRAM:

downloader.download_if_needed()

"    input_path="samples/ultrasound.jpg",  - `QWEN_N_GPU_LAYERS=12` (máº·c Ä‘á»‹nh, 4GB VRAM nÃªn Ä‘á»ƒ 8â€“16; Ä‘áº·t 0 Ä‘á»ƒ cháº¡y CPU-only)

```

    output_path="output/baby.png",  - `QWEN_N_THREADS=<sá»‘ luá»“ng CPU>`; `QWEN_N_CTX=2048`

### Memory Issues

- Use CPU mode if GPU memory is insufficient    backend="auto",  # hoáº·c "qwen", "qwen_gguf"- YÃªu cáº§u cÃ i `llama-cpp-python` (Ä‘Ã£ cÃ³ trong `requirements.txt`). Cáº§n báº£n llama.cpp há»— trá»£ multi-modal (VL). Náº¿u backend GGUF chá»‰ tráº£ vá» vÄƒn báº£n thay vÃ¬ áº£nh, code sáº½ tá»± Ä‘á»™ng dÃ¹ng vÄƒn báº£n Ä‘Ã³ lÃ m prompt tÄƒng cÆ°á»ng cho ControlNet Ä‘á»ƒ váº«n táº¡o Ä‘Æ°á»£c áº£nh.

- Close other applications to free RAM

- Try 4GB mode instead of 12GB mode    ethnicity="mixed ethnicity"



### CUDA Issues)## Cáº¥u trÃºc thÆ° má»¥c (Ä‘Ã£ cÆ¡ cáº¥u láº¡i)

```bash

# Check CUDA availability```- `src/babyvis/`: mÃ£ nguá»“n chÃ­nh (inference, model_utils)

python3 -c "

import subprocess- `apps/`: Ä‘iá»ƒm cháº¡y

try:

    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)## ğŸ”§ Cáº¥u hÃ¬nh  - `gradio_app.py`: giao diá»‡n Gradio

    print(result.stdout)

except:  - `batch_processor.py`: xá»­ lÃ½ theo lÃ´

    print('CUDA not available, will use CPU')

"### Environment Variables- `web/`: giao diá»‡n web tÄ©nh (index.html, style.css, app.js) â€” Ä‘Ã£ loáº¡i bá» toÃ n bá»™ icon SVG

```

```bash- `outputs/`: áº£nh káº¿t quáº£ (`gradio/`, `batch/`, `tmp/`)

## ğŸ“ License

# Backend selection- `samples/`: áº£nh máº«u

See LICENSE file for details.

export QWEN_BACKEND=auto          # auto, qwen, qwen_gguf- `config/`: cáº¥u hÃ¬nh batch (`config/batch_config.json`)

## ğŸ¤ Contributing

export USE_QWEN_IMAGE_EDIT=1      # Force transformers- `data/`: dá»¯ liá»‡u vÃ o (vd. `data/image_list.txt`)

This is a simplified GGUF-only version focused on ease of use and performance.
export USE_QWEN_GGUF=1            # Force GGUF- `logs/`: log cháº¡y batch



# Performance tuningGhi chÃº: CÃ¡c script trong `apps/` tá»± thÃªm `src/` vÃ o `PYTHONPATH` khi cháº¡y trá»±c tiáº¿p, nÃªn khÃ´ng cáº§n cÃ i Ä‘áº·t gÃ³i.

export QWEN_4BIT=true             # 4-bit quantization

export QWEN_FLASH_ATTN=true       # Flash attention## Báº£o trÃ¬ vÃ  lÆ°u Ã½

export QWEN_N_GPU_LAYERS=20       # GPU layers for GGUF- Repo Ä‘Ã£ Ä‘Æ°á»£c dá»n dáº¹p vÃ  loáº¡i bá» táº¥t cáº£ icon (SVG/biá»ƒu tÆ°á»£ng) khá»i giao diá»‡n tÄ©nh.

- HÃ¬nh áº£nh sinh ra chá»‰ mang tÃ­nh tham kháº£o, khÃ´ng thay tháº¿ Ã½ kiáº¿n y khoa.

# Local models
export QWEN_LOCAL_PATH=/path/to/local/model
export QWEN_GGUF_PATH=/path/to/model.gguf
```

### Batch Processing
```python
from apps.batch_processor import BatchImageProcessor

processor = BatchImageProcessor()
results = processor.process_image_list([
    "samples/1.jpeg",
    "samples/2.png",
    "samples/3.jpg"
])
```

## ğŸ¯ Backends

| Backend | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Khuyáº¿n nghá»‹ |
|---------|---------|------------|-------------|
| **Auto** | Tá»± Ä‘á»™ng chá»n tá»‘t nháº¥t | - | âœ… **Default** |
| **GGUF** | TÆ°Æ¡ng thÃ­ch cao, Ã­t VRAM | Cháº­m hÆ¡n | Personal PC |
| **Transformers** | Cháº¥t lÆ°á»£ng cao, nhanh | Nhiá»u VRAM | Server/GPU máº¡nh |
| **CPU** | KhÃ´ng cáº§n GPU | Ráº¥t cháº­m | Backup mode |

## ğŸ“Š Hiá»‡u suáº¥t

### VRAM Requirements
- **4GB**: GGUF Q4_K_M, 4-bit quantization
- **6GB**: GGUF Q5_K_M, optimal balance  
- **8GB+**: Transformers full precision
- **10GB+**: Flash attention enabled

### Tá»‘c Ä‘á»™ Æ°á»›c tÃ­nh
- **GGUF CPU**: ~60-120s/image
- **GGUF GPU**: ~15-30s/image  
- **Transformers GPU**: ~5-15s/image

## ğŸ” Troubleshooting

### Common Issues

**CUDA errors:**
```bash
export FORCE_CPU=1
./run_qwen_cpu.sh
```

**Out of memory:**
```bash
export QWEN_4BIT=true
export QWEN_8BIT=true
./run_qwen_gguf.sh
```

**Model not found:**
```bash
# Thá»­ cÃ¡c repo fallback
export QWEN_GGUF_REPO=bartowski/Qwen-Image-Edit-GGUF
./run_qwen_gguf.sh
```

## ğŸ“ Project Structure

```
BabyVis/
â”œâ”€â”€ src/babyvis/           # Core Qwen processing
â”‚   â”œâ”€â”€ model_utils.py     # Model loaders & VRAM detection
â”‚   â””â”€â”€ inference.py       # Image generation pipeline
â”œâ”€â”€ apps/                  # Applications
â”‚   â”œâ”€â”€ batch_processor.py # Batch processing
â”‚   â””â”€â”€ gradio_app.py      # Web interface
â”œâ”€â”€ samples/               # Sample images
â”œâ”€â”€ outputs/               # Generated results
â””â”€â”€ run_qwen_*.sh         # Execution scripts
```

## ğŸ¨ TÃ¹y chá»‰nh

### Custom Instructions
```python
generate_predict_qwen_only(
    input_path="input.jpg",
    output_path="output.png",
    instruction="Transform this ultrasound into a realistic Asian newborn baby photo"
)
```

### Quality Settings
```python
# High quality (server)
export QWEN_N_CTX=4096
export QWEN_FLASH_ATTN=true

# Balanced (personal)  
export QWEN_N_CTX=2048
export QWEN_4BIT=true

# Fast (minimal)
export QWEN_N_CTX=1024
export QWEN_N_GPU_LAYERS=8
```

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch: `git checkout -b feature/amazing-feature`
3. Commit changes: `git commit -m 'Add amazing feature'`
4. Push to branch: `git push origin feature/amazing-feature`
5. Open Pull Request

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file.

## ğŸ™ Acknowledgments

- **Qwen Team**: Qwen Image Edit model
- **llama.cpp**: GGUF inference engine
- **Hugging Face**: Model hosting & transformers
- **CUDA/PyTorch**: GPU acceleration

---

**ğŸš€ Ready to generate baby predictions with Qwen AI!**

For support: Create an issue or contact [@VuThanhLam124](https://github.com/VuThanhLam124)